Plasma Architecture  Sarath Lakshman  March 22, 2017 Overview  •Background  •Design Considerations  •Architecture  •Enhancements  •Conclusion Background  •Memory Optimized Indexes rocks!  –Scale performance linearly with more CPU cores,  DRAM  –Low latency fast in-memory snapshots every 20ms  –Lock-free data structures everywhere leverages  many readers and writers  –Persistence using non-intrusive full snapshot  backups  –Highly optimized to avoid the cost of back-index  (cpu and memory)  Background  Nitro Storage Engine  •Create backups from snapshots and  recover nitro after restart/crash  •Free items when GCed and not in  reference  •Remove items from skiplist which belongs  to the unused snapshots  •Create point-in-time immutable snapshots  for index scans  •Avoid phantoms and provide scan stability  •Manage index snapshot versions in use  •Implements Insert, Delete, Lookup, Range  Iteration  •Concurrent partitioned visitors  •Concurrent bottom-up skiplist build  Background  •Skiplist  Lockfree linked list  100  Head   20 50 Insert 50 between 100 and 20 Lockfree linked list  100  Head   20 50Step 1  Lockfree linked list  100  Head   20 50 Step 2  CAS(&Ptr100, Ptr20, Ptr50)  If CAS fails,  restart the entire operation again! Lockfree linked list  100  Head   20 50 Success!  * All the linked list operations illustrated in the upcoming slides are atomic linked list operations Background  •Nitro MVCC  V=10 bornSn=1  deadSn=0  V=20 bornSn=1  deadSn=0  V=30 bornSn=1  deadSn=0  Create Snapshot (Sn=1)  V=10 bornSn=1  deadSn=0  V=20 bornSn=1  deadSn=0  V=30 bornSn=1  deadSn=0  V=15 bornSn=2  deadSn=0  V=32 bornSn=2  deadSn=0  Create Snapshot (Sn=2) Lifetime (bornSn, deadSn) Background  •Nitro MVCC Recap  Create Snapshot (Sn=3) V=10 bornSn=1  deadSn=0  V=20 bornSn=1  deadSn=3  V=30 bornSn=1  deadSn=0  V=15 bornSn=2  deadSn=0  V=32 bornSn=2  deadSn=3  V=32 bornSn=3  deadSn=0  V=10 bornSn=1  deadSn=0  V=20 bornSn=1  deadSn=3  V=30 bornSn=1  deadSn=0  V=15 bornSn=2  deadSn=0  V=32 bornSn=2  deadSn=3  V=32 bornSn=3  deadSn=0 Background  •Nitro MVCC Recap  Visibility: Iterator (Sn=1) V=10 bornSn=1  deadSn=0  V=20 bornSn=1  deadSn=3  V=30 bornSn=1  deadSn=0  V=15 bornSn=2  deadSn=0  V=32 bornSn=2  deadSn=3  V=32 bornSn=3  deadSn=0  V=10 bornSn=1  deadSn=0  V=20 bornSn=1  deadSn=3  V=30 bornSn=1  deadSn=0  V=15 bornSn=2  deadSn=0  V=32 bornSn=2  deadSn=3  V=32 bornSn=3  deadSn=0 Background  •Nitro MVCC Recap  Visibility: Iterator (Sn=2) V=10 bornSn=1  deadSn=0  V=20 bornSn=1  deadSn=3  V=30 bornSn=1  deadSn=0  V=15 bornSn=2  deadSn=0  V=32 bornSn=2  deadSn=3  V=32 bornSn=3  deadSn=0  V=10 bornSn=1  deadSn=0  V=20 bornSn=1  deadSn=3  V=30 bornSn=1  deadSn=0  V=15 bornSn=2  deadSn=0  V=32 bornSn=2  deadSn=3  V=32 bornSn=3  deadSn=0 Background  •Nitro MVCC Recap  V=10 bornSn=1  deadSn=0  V=20 bornSn=1  deadSn=3  V=30 bornSn=1  deadSn=0  V=15 bornSn=2  deadSn=0  V=32 bornSn=2  deadSn=3  V=32 bornSn=3  deadSn=0  Visibility: Iterator (Sn=3) Background  •Memory Optimized Index++  –Fully memory resident is expensive (Best option  for predictable size index with random access)  –A cache solution that operates between DRAM  and Flash is more desirable  –Full snapshot persistence is expensive as it has  significant impact on SSD lifetime  –Thoughts on extending Nitro  - ended up building  another storage engine based on same principles Design Considerations  •Scalability  –Scale storage engine subsystems almost linearly  •Multicore CPUs  –Lock-free data structures  •Memory first architecture  –Persist only when required  •Flash/SSD trends  –Very good read performance  –Log structured append only storage  •Fast in-memory snapshots  •Ability to Rollback  Architecture  Page  Delta Linked list  Base sorted list  Insert  - Index layer  Insert 1000 Insert  - Index layer  Insert 900  Insert 1000 Insert  - Index layer  Insert 550  Insert 900  Insert 1000 Delete  - Index layer  Delete 1000  Insert 550  Insert 900  Insert 1000 Insert  - Index layer  Insert 100  Delete 1000  Insert 550  Insert 900  Insert 1000 Insert  - Index layer  Number of delta operations > Compaction Threshold  Let’s Compact  Insert 100  Delete 1000  Insert 550  Insert 900  Insert 1000 Compaction  - Index layer  100 550 900Insert  - Index layer  100 550 900 Insert 200  Insert 1000  Insert 600  Split  - Index layer  100 200 550 600 900 1000 Page has too many items!  Let’s split the page  Insert 250  Split  - Index layer  100 200 550 600 900 1000  Insert 250  600 Split - 600  600 900 1000  Not yet linked to  Index layer Split  - Index layer  100 200 550 600 900 1000  Insert 250  600 Split - 600   600 900 1000  Linked to the index layer Merge  - Index layer  100 200 250 550 600 900 Page is under filled  Let’s merge Merge  - Index layer  100 200 250 550 600 900 Soft-Remove  Writer threads will avoid  modifying this page as it  observes soft-remove delta  The corresponding thread  should complete merge  operation before inserting Merge  - Index layer  100 200 250 550 600 900 Soft-Remove   Merge 600  Merge  - Index layer  100 200 250 550 600 900 Soft-Remove   Merge 600  Unlinked from Index layer  Skiplist node is removed Range queries  100 100 550 900 Insert 200  Insert 1500  Insert 600  10 10  15  19 Insert 20  Query = Range(200, 1000) Range queries  100 100 550 900 Insert 200  Insert 1500  Insert 600  Query = Range(200, 1000)  Sort Merge   200, 550, 900 Persistence  20 100 550 900 Insert 200  100 550 900 Insert 200 Pid = 20  Marshal Page - >  Append-only log file  tailOffset = 100  0 100Size = 300 bytes Persistence  100 550 900 Insert 200 Pid = 20 Append-only log file  0 100 400 20 100 550 900 Flush offset 100  Insert 200  Persistence  100 550 900 Insert 200 Pid = 20 Append-only log file  0 400 20 100 550 900 Flush offset 100  Insert 200  0  1 Insert 10 Pid = 0  600 Insert 250  Insert 150  Persistence  100 550 900 Insert 200 Pid = 20 Append-only log file  0 400 20 100 550 900 Flush offset 100  Insert 200  0  1 Insert 10 Pid = 0  Insert 250  Insert 150  600 Flush offset 600  700 Insert 150 Pid = 20  Insert 250  Next Offset = 100  Page Transactions  0 100 200 550 600 900 1000  Insert 250  600 Split - 600  600 900 1000  Pid=0  Range: 0 - 1500  Log Pid=0  Range: 0 - 600  Pid=600  Range:  600-1500  Crash! Concurrent Log Writes  •A lockfree write buffer of size 1MB (SSD friendly)  •Concurrent threads can reserve allocations in the  log at an offset  •Once all concurrent writers finish copying to the  write buffer, the last writer calls pwrite() with 1MB  chunk  •A thread can read from the write buffer as well (If  a flush delta with an offset-not-yet on log exists in  a page)  •Multiple writer buffers are used alternately to  allow pipelining while pwrite() blocking Write Buffer Reservation  100 550 900 Insert 200 Pid = 20  0 400 0  1 Insert 10 Pid = 0  600tailOffset=600  600 1049176  20 100 550 900 Flush offset 650  Insert 200  Log GC/Cleaning  •Log structured file system style log garbage  collection  •Start from head of the log, relocate valid pages  to the the end  •Trim the log at GC’ed offset  •The log is always growing at the tail  •Incremental log GC based on fragmentation  threshold Log GC  0 Flush offset 60  0  V=1 10 Flush offset 10  20 Flush offset 70  30 Flush offset 40  10  V=2 0  V=1 20  V=5 30  V=1 10  V=2 0  V=2 20  V=6 0 1020304050607080v=2 v=2 v=6 v=1 Head  Log GC  0 Flush offset 60  0  V=1 10 Flush offset 10  20 Flush offset 70  30 Flush offset 40  10  V=2 0  V=1 20  V=5 30  V=1 10  V=2 0  V=2 20  V=6 0 1020304050607080v=2 v=2 v=6 v=1 Head  Log GC  0 Flush offset 60  0  V=1 10 Flush offset 10  20 Flush offset 70  30 Flush offset 40  10  V=2 0  V=1 20  V=5 30  V=1 10  V=2 0  V=2 20  V=6 0 1020304050607080v=2 v=6 v=1 Head  10  V=3 Reloc offset 80  v=3 100Log GC  0 Flush offset 60  0  V=1 10 Flush offset 10  20 Flush offset 70  30 Flush offset 40  10  V=2 0  V=1 20  V=5 30  V=1 10  V=2 0  V=2 20  V=6 0 1020304050607080v=2 v=6 V=2 Head  10  V=3 Reloc offset 80  v=3 100 30  V=2 Reloc offset 100  110 Log GC Trim  0  V=1 10  V=2 0  V=1 20  V=5 30  V=1 10  V=2 0  V=2 20  V=6 0 1020304050607080 10  V=3 100 30  V=2 110Head  Log GC Trim  0  V=2 20  V=6 607080 10  V=3 100 30  V=2 110Head  Log implementation  •Plasma Log is implemented as a collection of fixed size files  of 4GB  •Each file chunk represents a range of log address of 4GB  •Log has a superblock file which tracks current  logHeadOffset, logTailOffset  •Superblock has two 4KB blocks to store metadata with  checksum  •If crash occurs during a write/fsync, log init will use the  correct logHead, logTail offset by verifying the checksum for  two blocks  •Log Trim command deletes a log file chunk when GCHead  moves beyond 4 GB alignment  •The 64 bit offsets can live for 500 years (250 MB/s)  Log implementation  Block-1  checksum  Block-0  checksum  Superblock  File-0   File-1   File-2   File-3  0 4gb 8gb 12gb 14gb Trim operation can be implemented using file delete  For Linux, a Linux fs holepunching based single file log implementation (optional) Lockfree file segment reclamation  •The GC will move headOffset after cleaning  •We cannot trim the log until all readers have moved  off the files-to-be-deleted  •Reference counting with file locks will become single  point of contention  •Pages can be partially on Flash or Memory. Hence we  have to to-be-deleted files for DB access  •Instead of locking, we implement a lockfree algorithm  (SFR – Safe File Reclamation) to track potential reader  access to a file and remove them safely.  •Based on ideas from EPOCH based memory  reclamation algorithm  Page swapping  •When there is memory pressure, background  swapper evicts pages from memory  •A read on evicted page will bring the page  back into the cache  •An insert into an evicted page does not  require a page swapin. Writes can can append  insert/delete delta into the page similar to  LSM trees Page swapping  20 100 550 900 Flush offset 100  Insert 200  1000  1000  1001  1002  Insert 1500  Flush offset 50  Page swapping  20 Swap offset 100  1000  1000  1001  1002  Insert 1500  Flush offset 50  100Page swapping  20 Swap offset 100  1000  Swap offset 200  100200Page swapping  20 Swap offset 100  1000  Swap offset 200  100200 Insert 2500  Page swapping  20 Swap offset 100  1000  Swap offset 200  100200 Insert 2500  Read 1001  1000  1001  1002  Swapin 200  Cache Management  •Clock: Approximate LRU Algorithm  •A concurrent variant of Clock is implemented  on top of lock-free skiplist index layer Crash Recovery  •The index layer is rebuild by replaying the log  from head to tail  •In future, Nitro style periodic backup on index  layer can performed for facilitating instant  recovery  Snapshots  •Nitro style MVCC is implemented to provide  point-in-time snapshots  •Every Insert/Delete operation is tagged with 8  byte snapshot number  •Entries belonging to dead snapshots are lazily  garbage collected as part of page compaction  operation Persistent Snapshots/Rollback  •Plasma allow to create recovery points on a  Plasma snapshot  •It is non-blocking an runs in the background  •GSI creates persistent snapshots every 10mins  •Two persisted snapshots are maintained to  make sure that we can always rollback atleast  10 mins old data Overhead of Persistent Snapshot  •GSI creates in-memory snapshots every 20ms  •It results in 30000 snapshots every 10 mins  •If we lock a snapshot, future garbage MVCC  garbage collections cannot be proceeded. ie., a  entry in snapshot 0 may be deleted in snapshot  10000. If we remove the item, a read from  snapshot 0 will miss entries and cause data loss  •For maintaining persistent snapshot, we have  preserve correctness of snapshot 0 MVCC Garbage Collection  Insert  100 Insert  200 Insert  150 Insert  500 Insert  1 Delete  500 Delete  150 Insert  50 Delete  1 Insert  20 Delete  20 Insert  5 Insert  300 Delete  5 Insert  1  Delete  50Persisted  snapshot  Persisted  snapshot Sn 0 Sn 1 Sn 2 Sn 3  Sn 4 Sn 5 Sn 6 Sn 7 Active readers Active readers MVCC Garbage Collection  Insert  100 Insert  200 Insert  150 Insert  500 Insert  1 Delete  500 Delete  150 Insert  50 Delete  1 Insert  20 Delete  20 Insert  5 Insert  300 Delete  5 Insert  1  Delete  50Persisted  snapshot  Persisted  snapshot Sn 0 Sn 1 Sn 2 Sn 3  Sn 4 Sn 5 Sn 6 Sn 7 Active readers Active readers MVCC Garbage Collection  Insert  100 Insert  200 Insert  150 Insert  500 Insert  1 Delete  500 Delete  150 Insert  50 Delete  1 Insert  20 Delete  20 Insert  5 Insert  300 Delete  5 Insert  1  Delete  50Persisted  snapshot  Persisted  snapshot Sn 0 Sn 1 Sn 2 Sn 3  Sn 4 Sn 5 Sn 6 Sn 7 Active readers Active readers MVCC Garbage Collection  Insert  100 Insert  200 Insert  150 Insert  500 Insert  1 Delete  500 Delete  150 Insert  50 Delete  1 Insert  20 Delete  20 Insert  5 Insert  300 Delete  5 Insert  1  Delete  50Persisted  snapshot  Persisted  snapshot Sn 0 Sn 1 Sn 2 Sn 3  Sn 4 Sn 5 Sn 6 Sn 7 Active readers Active readers MVCC Garbage Collection  Insert  100 Insert  200 Insert  150 Insert  500 Insert  1 Delete  500 Delete  150 Insert  50 Delete  1 Insert  20 Delete  20 Insert  5 Insert  300 Delete  5 Insert  1  Delete  50Persisted  snapshot  Persisted  snapshot Sn 0 Sn 1 Sn 2 Sn 3  Sn 4 Sn 5 Sn 6 Sn 7 Active readers Active readers MVCC Garbage Collection  Insert  100 Insert  200 Insert  50 Insert  5 Insert  300 Delete  5 Insert  1  Delete  50Persisted  snapshot  Persisted  snapshot Sn 0 Sn 1 - 4  Sn 5 Sn 6 Sn 7 Active readers Active readers MVCC Garbage Collection  •We eliminate this problem by performing a special  visibility aware MVCC garbage collection within  these 30000 snapshots  •BigIdea: We know that snapshot 0 will be used in  the future, but snapshot 1- 2910 are not going to  be used and hence invisible snapshots. We  garbage collect by merging the snapshots 1-2910  (Significant memory and storage savings)  •We keep atmost n+1 versions of a doc’s entry (n =  no of persisted snapshots) Rollback  •Add rollback delta which invalidates future  snapshots  100 Insert 1500/sn=1000  Insert 2000/sn=750  Insert 500/sn=500  Rollback sn:700 - 1200  Memory Management  •Plasma integrates with Nitro Safe Memory  Reclaimer  •JEMalloc is used for allocations Key Compression  1000  Insert  1200  Sn=100  Delete  1000  Sn=30  Insert  1000  Sn=20  Delete  1000  Sn=18  Insert  1000  Sn=2 Key Compression  1000  Insert  1200  Sn=100  Delete  Sn=30  Insert  Sn=20  Delete  Sn=18  Insert  1000  Sn=2 Same technique can be extended to compress  Index entries with high cardinality emitted  from different documents  [Bangalore, doc-100]  [Bangalore, doc-1]  [Bangalore, doc-200]  [doc-100]  [doc-1]  [Bangalore, doc-200] Default configuration  0 100 550 900 Flush offset 100  Insert 200  Insert 250  Flush offset 150  Compaction threshold = 200  Split threshold = 400  Merge threshold = 25  0  V=1 10  V=2 0  V=1 20  V=5 0  V=1 10  V=2 0  V=1 20  V=6 Flush offset 50   Max page segments Highlights  •Concurrent Incremental persistence  –Significantly reduces write amplification  •Avoids wandering tree problem  –COW B+ tree is expensive  –Plasma writes only affects data layer pages  •Incremental log garbage collection  –Avoids writer vs compactor catch-up problem  –Predictable write performance  •Allows write/read optimization tradeoffs  –Tunable delta length, page split/merge thresholds and max  page segments allows to tradeoff performance factors  appropriately for cpu vs memory vs write amplification  Future Enhancements  •Prefix compression  –Each page has a fixed low and high key  –This property allows Plasma to avoid duplicating  common page key prefix across all the keys  –This can significantly reduce storage requirements  for Couchbase N1QL universal Index Prefix compression  [SFO A]  [SFO A]  [SFO D ]  [SFO E]  Insert [SFO, B]  [SFO H]  [SFO J]  [SFO K]  [SFO N]  Insert [SFO, I]  KeyRange: ([SFO A], [SFO H]) KeyRange: ([SFO H], [SFO P]) Prefix compression  [SFO A]  [A]  [D ]  [E] Insert [B]  [SFO H]  [J]  [K]  [N] Insert [I]  Compressed Pages KeyRange: ([SFO A], [SFO H]) KeyRange: ([SFO H], [SFO P]) Future Enhancements  •Fast range count/Limited reductions  –Every insert/delete into the page can track count for  an MVCC snapshot with O(1) operation  –Page compactions will be aware of the snapshot level  counts for the page  –A range count query on a large range can fully cover  several pages. Count can be retrieved without  reconstructing the ordering of items for fully covered  pages  –Using large page sizes can significantly speed up count Count/Limited reductions  20 100  550  900 Insert 200  Delete 1000  Insert 600  Sn=50 C=4  Sn=8 C=4  Sn=12 C=3  Sn=1 C=1   Sn=5 C=3 Count/Limited reductions  0  Page   Page   Page   Page  1000   2000   3000  RangeCount(800, 3002)  Fully covered  Future Enhancements  •Reverse Iterator  –Skiplist Index layer has only forward pointer  –The challenge is to efficiently iterate in reverse  order without using any prev node linked list  Future Enhancements  •A hash table Index  implementation based on  the same ideas  •Significantly improve GSI BackIndex  performance for Plasma  •GSI Hash Index HT Design  b0 b1 b2 b3 HashTable buckets  Key 8   Key 0   Key 4  Key 5  Key 3   Key 11  Key 2  H(key) % 4 == 0  H(key) % 4 == 1  H(key) % 4 == 2  H(key) % 4 == 3  Index Layer Data layer Linear Hashing  b0 b1 b2 b3 Key 8   Key 0   Key 4  Key 5  Key 3   Key 11  Key 2  H(key) % 8 == 0  H(key) % 4 == 1  H(key) % 4 == 2  H(key) % 4 == 3  b4 Key 4  Bucket split! Plasma HashTable  b0 h(key) % 4 == 0  Insert 8  b1 h(key) % 4 == 1  5  b2 h(key) % 4 == 2  Insert 2  b3 h(key) % 4 == 3  3   11 0  4 Future Enhancements  •Non-volatile Memory (Intel 3DX point)  –Plasma log can be implemented based on  pmem.io APIs  –Plasma configuration can be tuned to leverage  high random read capability of pmem devices Hot/Cold classification  •It is common to have majority idle documents  in OLTP workload  •Classify index into two stores, Hot and Cold  •Allows to reduce write amplification in  random workload  Conclusion  •Plasma presents a highly scalable KV storage  engine  •Plasma allows various tunable tradeoffs for  performance, cpu, memory and SSD write  amplification  •Plasma features a memory first write  optimized, read tunable design  Scalable Log Management in Plasma Overview Scalable Log Design Log Segments Concurrent Garbage Collector Segment Summary Block Garbage Aware Cleaning Log segment metadata management Log chain management Data size accounting Garbage Collection Safe Log Segment Reclamation Checkpointing Crash Recovery Future in-place update scheme on segments Multiple logs for Hot/Cold page separation Faster validity check for value separation Backward compatibility and upgrade Overview The Plasma storage engine uses log structured storage model for storing data on persistent media. Data updates are always appended to the tail of the log. Update to a data page results in writing a new page segment and potentially making the older page segment versions garbage. The latest version of the pages on the log are referenced by the index structures and older versions become garbage. Plasma uses a copying garbage collector to clean the log by starting from the head of the log towards the tail of the log. For a disk oriented workload with no write caching, the log cleaner will become the bottleneck for scaling Plasma. We discuss the three major issues related to log cleaning in Plasma. Problem 1: Single threaded log cleaner A single thread is used for performing the log cleaning operation. When data size is higher than the available memory, there is no opportunity for performing write caching to reduce the write amplification. Hence, write amplification is high as well as the number of page segment blocks written to the log is going to be higher. Plasma writers can almost scale linearly by increasingthe number of writer threads. The single threaded log cleaner thread fails to catch-up cleaning the garbage generated by many writer threads for a disk oriented workload. Problem 2: Read bandwidth usage The log cleaner reads from the head of the log towards the tail of the log. It evaluates the blocks in the log for valid blocks. If a valid block is found, it rewrites the page corresponding to the block to the tail of the log. To evaluate a block for its validity, it requires only the pageID for the page in the block. But, currently the log cleaner reads the entire block to obtain the PageID consuming a large chunk of the available read bandwidth. Problem 3: Sequential cleaning The log cleaning in Plasma follows cleaning from head towards tail of the log, essentially cleaning the old data towards recent data. This model works great for a data update workload which is uniformly distributed. But for a workload where data updates are skewed and focussed to a small subset such as 1%, cleaning becomes very inefficient. The cleaning will have to move a large chunk of valid data to clean the garbage. The data movement consumes significant read and write bandwidth. Problem 4: Expensive item validation during cleaning for value separation (Non GSI use case) When values are separated from pages, the page holds a value offset pointers along with the item key instead of storing the value along with the key. During log cleaning, it has to verify if the value if valid by doing a lookup operation in the page and check if any of the item’s value pointer points to the current value offset. When pages are mostly resident only on the log/disk, every item validity check requires reading the page from the log and it is expensive. This document aims at proposing scalable solutions to the above problems. Scalable Log Design Log Segments A log segment is a fixed size physical storage unit of the log. The log segment size should be in the multiples of SSD erase block size. A segments maps to a range of data offset in the log. The default size will be 1MB. A log is a linked list of segments. By introducing the segments to the log, we are able to independently operate on different log segments concurrently. The log contains variable size blocks. Hence, it is difficult to find the starting offset of a block by randomly reading from an offset in a log. Since a segment is fixed size, we can randomly pick two different segments for processing. Each segment could also store metadata information about the segment.The segments could contribute to internal fragmentation. The log writer consists of a tail flush buffer. The data is flushed to the log only when the flush buffer is full. But, when we need to do a synchronous log persistence, log segments may cause an extra padding block equal to the segment size. Synchronous writes are performed only during recovery point creation. Concurrent Garbage Collector The addition of log segments allows plasma to independently operate on different log segments as it eliminates the need for sequential block by block operation during log cleaning. We introduce a cleaner tail offset and a cleaner head offset. When the fragmentation threshold goes above the limit, any log cleaner worker can pick-up the cleaning work by atomically incrementing the cleaner tail by segment size. The cleaning work is straightforward and follows the same copying garbage collection technique used in the current implementation. The tricky part is to maintain the head and tail offset metadata of the log. Different workers concurrently operate on different segments and they operate at different speed. As the cleaning progresses, we need to update the head offset. If the thread processing segment-3 finishes before threads processing segment-2 or segment-1, we cannot update the log head as segment-3’s offset. It will lead to log corruption. The log head can be updated only in the sequential manner as it’s log offset. Even though segment-3 cleaning is completed, log head can be updated to segment-3 after finishing segment-1 and segment-2 cleaning. A reference count based efficient parent-child relationship can be build for the segments to implement sequential head offset update. Similar technique has been employed in the lock-free flush buffer implementation for log. Each segment should have a small metadata header to indicate the size of the segment. If a block stored in the log is greater than default 1MB size, this metadata indicates that it uses n number of segments. Segment Summary Block The log cleaner reads the log data from head of the log towards the tail. Log cleaner only looks at the PageID and the PageVersion from the lss blocks. PageID and PageVersion only consumes tiny fraction of the log space. Log reads for fetching the PageID and PageVersion consumes about 30% of the read bandwidth. The bandwidth consumption can be reduced significantly by adding a summary block to the log segments. The summary block will summarise the list of PageIDs and PageVersions for the blocks in the log segment. The key challenge is to maintain the summary block in a lock-free manner. All writes are performed into the lockfree flush buffer. Earlier, when a block is reserved in the lockfree flush buffer, the offset was incremented atomically with complex lockfree reference counting scheme. The lockfree buffer maintain a 64 bit state variable. The layout of the state is as follows: The log data can be stored at the head of the log segment and the summary can be stored at the tail of the segment. They can grow in opposite direction until they meet. Maximum space is limited by 1MB. Since we need to track both data portion and summary portion in the log segment, we need two offsets. DataOffset and SummaryOffset. Since the log segment size is now restricted to 1MB, we do not need 32 bits to represent the data. The state variable layout can be changed to accommodate summary block information as follows: The data offset and summary offset can represent up to 16MB log segment. Maximum number of accessors possible is 16384. The segment has a fixed size header and footer block. The header and footer stores the following information: 1. Size of data 2. Size of summary 3. Is the block an overflow block Plasma does not restrict the size of data stored in the log. If an object is bigger than segment size of 1MB, we have to store it using overflow log segments. When the log cleaner observes a log segment with overflow, the footer indicates to ignore that log segment for cleaning. Garbage Aware Cleaning The fundamental problem with sequential log cleaning is that it will have to perform lot of additional work when large portion of the data is cold and a small percent is hot data. In the following diagram, the log is 20 % fragmented. But the garbage data (red blocks) are in the middle of the log. Now, in order to reclaim space, we have to read and relocate 50% of the existing data in the log. This consumes significant read and write bandwidth. Garbage aware cleaning aims at a new design for log storage by adding capabilities for granular garbage collection. Earlier sections of this document discusses about splitting the log into log segments. Instead of cleaning the log segments in the order in which they were created, the cleaning is performed on log segments which have got the highest garbage. This significantly improves the storage bandwidth utilization. The garbage aware cleaning requires checkpointing in Plasma for it to work. This design also enables Plasma to work directly on a block device and avoids any need for a filesystem. The log segments are physical unit of the log and they have a starting and ending physical offsets. The log segments are connected in the order of their writes in a chain called log chain. Log segment metadata management Each log segment has a metadata object held in memory. The metadata consists of: 1. Valid data size 2. Valid item count 3. Total item count 4. Seqno 5. State information (is_free, cleaning) Each log segment is identified by the logID. The log segments can be stored in single file or multiple files. The logIDs are generated by using a combination of file number and file offset. To locate the in memory metadata object for a log segment, the metadata objects are stored in chunks of blocks of memory directly addressable by performing modulo operation. Using virtual memory reservations, we could potentially reserve a large memory area without allocating memory to those locations. Even though virtual memory reservation based technique is optimal and easy, we need a platform independent implementation. The challenge here is that the log can grow based on the total data growth. Before initializing the log, we do not know the maximum size of the log to preallocate the log segment metadata object arrays. Even if we estimate the total, we cannot afford to allocate memory for the future growth. We need the metadata memory usage to grow along with the data size. We can estimate the maximum possible data size to be 100TB. We can implement a sparse index structure (BTree like) in memory as follows:If we limit the maximum log space to 100TB, we can build an array index with 100 entries pointing to the metadata object arrays (M0..Mn). Each entry will account for 1TB of log space. If the metadata object for the log is 32 bytes, 1TB log space requires 32MB of metadata. Initially only index 0 points to the log metadata array with 32MB in size. Once the log space grows above 1TB, a new metadata array of 32MB will be allocated and index position 1 will be filled. The lazy allocation approach limits the memory growth for the metadata objects. Log chain management The log segments are chained together in the order of their writes in a log chain. When a log segment is written to the durable storage, the next log in the chain is decided and written to the header metadata of the log segment being written. This enforces log chain durability. When a log segment is written to the durable storage, it is assigned a monotonically incrementing sequence number. All the log segments in the chain can be ordered by the sequence number. When a new log segment needs to be allocated, the log writer will lookup the log segment free pool or create a new log segment if free pool is empty. Free log segments will be fed into free pool after garbage collection of log segments. Data size accounting The valid data size is tracked in each of the flush delta in the page. For store level accounting, global data size counter is maintained. When a page segment becomes stale, the size is decremented from the global counter. With log segment metadata, instead of store level flush data accounting, data size will be accounted in the log segment metadata. This introduces some additional requirements. When a page segment needs to be marked as stale, the data size should be decremented on the log segment metadata where the page segment resides. Currently, we do not maintain individual offsets of each of the each of the page segments. We need to additionally maintain sizes and offsets for all the page segments to correctly account for the data size. This adds extra memory overhead for swapout delta. Swapout delta needs to track all the page segment sizes and its offsets. Garbage Collection A background gc worker scans the metadata objects for all the segments periodically and generates a list of segments ordered by highest amount of garbage. The log cleaning workers picks up the segments and clean them and put back to the free segments pool. After cleaning a segment, the segment cannot be put back to the free segments pool. During the relocation operations performed during the cleaning, it will be written to the lockfree flushbuffer of the tail of the log chain. If we happen to reuse the block that just got cleaned, we may overwrite the data on the durable storage before the old versions of the data blocks are relocated and rewritten to the durable storage. Hence, the segment should be marked as in use and marked and moved to free pool when the relocated segment is written to the durable storage. When a segment is cleaned and moved to the free pool, the log chain is broken. This will become an issue during crash recovery. This problem can be eliminated by using checkpoints for recovery. We need to maintain log chain only from the point of checkpoint. The log chain relationship of the segments does not need to be maintained any segments written before the checkpoint. The garbage collector should only consider the segments with seqno less than the current checkpoint sequence number as the candidates for garbage collection. Otherwise, we won’t be able to maintain the log chain. Safe Log Segment Reclamation When garbage collector cleans a segment, the segment cannot be immediately put back into the free pool. Since all the operations happen in lockfree manner, the segment needs to be parked in the safe memory reclaimer and put back to free pool once the reclaim session has ended.Checkpointing During checkpointing, the current logID needs to be stored. It will be the head log segment for log chain used for log replay during the recovery. Once a checkpoint is created, the garbage collection segment sequence number can be moved the checkpoint sequence number. Crash Recovery During recovery, Plasma needs to perform checkpoint based recovery. The log segment metadata structures needs to be rebuilt from each page during the recovery. The checkpointing also should store all the segment offsets and their sizes for each of the pages. Once index layer is rebuild after checkpoint, log chain replay has to be performed. One of the challenge for log chain is to identify the end of the log chain segment. Since we store log segment sequence number, it can be used to detect the end. The end segment would be the next segment in the chain having seq number not equal to curr_seq+1. All the segments which are not referenced during the recovery will be added to the free segments pool. Future in-place update scheme on segments The segment based cleaning approach also enables to perform inplace updates to the segments with garbage. If we add sufficient additional metadata/bitmap per segment metadata object, we could perform inplace updates and avoid cleaning. The writer could choose a segment with garbage and overwrite the data and reuse it. This technique called threading can be found in log structured file systems. A hybrid log management scheme can be used. If the log is running above configured threshold fragmentation, the writers could perform inplace update writes if the cleaning is expensive and consumes more bandwidth. Multiple logs for Hot/Cold page separation Instead of keeping all pages to a single log, we could implement multiple logs and categorize them into hot and cold logs. When a segment is cleaned, all the survivor blocks are the blocks which are long lived. Garbage blocks have been rewritten to the tail of the log at some point. Survivor blocks can be treated as cold blocks and they can be moved to a different log based on simple statistics. This enables classification of hot and cold data blocks to different log and reduces the log fragmentation.Faster validity check for value separation Each log segment has an in-memory metadata object consisting of information required for performing efficient garbage aware cleaning. When a value needs to be checked against its validity, current scheme involves performing a lookup into the page with item key and verify if the value pointer for the item in the page points to the current value offset. This may involve disk reads when there is only little memory available for caching and it is expensive. The item validity check can be significantly improved by using a bitmap in memory. Let’s call it validation bitmap. A segment is written to disk only when it is full. When it is ready to be written, we know exactly how many value objects have been written to the current log segment. We can construct a value bitmap (1 bit per value) and store it in the in-memory segment object. When a value is written, we store the position information. Ie., n - for an nth value in the segment. When a value is garbage collected from the page, it locates the segment in-memory meta object from the offset and sets the bitmap atomically. Position of the bit in bitmap is obtained from the value pointer. When the log cleaner runs, it uses the bitmap to check validity of the values instead of performing a lookup into the page. If the 1MB segment contains 1024 1KB values, it would require 128 bytes to store the bitmap. ie., 128 MB per 1TB. During checkpointing, the bitmap can be persisted to the disk. During the log replay after recovery, deleted objects after checkpointing and be incrementally updated. Implementation can take lazy approaches to update the bitmap. If a garbage collected value is not marked in the bitmap, it does not cause any functional incorrectness. It would need an additional page lookup to validate the value. Backward compatibility and upgrade The new log format and management breaks backward compatibility with the existing log management scheme. We will introduce this scheme only for newly created indexes. Confidential and Proprietary. Do not distribute without Couchbase consent. © Couchbase 2021. All rights reserved.  Secondary Index Storage  Plasma - Design to Scale  Confidential and Proprietary. Do not distribute without Couchbase consent. © Couchbase 2021. All rights reserved. Presented by Durga Akhil Mundroy  SESSION FOCUS: Global Secondary Index Agenda  Fill in agenda items and numbers  Plasma Design  Memory Management  Log File Management  I/O Improvements and Recovery 01/ 01/ 02/ 03/ 04/ 05/Scaling  Stats 06/Confidential and Proprietary. Do not distribute without Couchbase consent. © Couchbase 2021. All rights reserved. 31Plasma Design Confidential and Proprietary. Do not distribute without Couchbase consent. © Couchbase 2021. All rights reserved. 4 Confidential and Proprietary. Do not distribute without Couchbase consent. © Couchbase 2021. All rights reserved. 4 Confidential and Proprietary. Do not distribute without Couchbase consent. © Couchbase 2021. All rights reserved. 4•Storage Engine for Secondary Indexing for Range Scan and Point Lookup  •Proprietary lock-free data structure enables highly concurrent index updates  •Use snapshot to eliminate locking during concurrent range scan  •Least-Recently-Used like eviction algorithm for memory management  •Concurrent design enables high throughput on scan workload     with high locality  •Use append-only Log Structured Persistent Storage (LSS)     optimized for SSD/NVME  •Support Incremental Compaction on LSS  •Use checkpointing for fast recovery  •Auto tune based on workload, available memory and index key sizes Plasma Design  Architecture Overview Confidential and Proprietary. Do not distribute without Couchbase consent. © Couchbase 2021. All rights reserved. 52Memory Management Confidential and Proprietary. Do not distribute without Couchbase consent. © Couchbase 2021. All rights reserved. 6 Confidential and Proprietary. Do not distribute without Couchbase consent. © Couchbase 2021. All rights reserved. 6•Memory usage comes from:  •Cached index keys  •Internal data structure  •Metadata  •Buffers  •Internal memory fragmentation  •Quota limits maximum memory use for index keys  •Honor quota on best effort basis  •Evict data from memory to adhere to the quota  •Cause index resident percent to reduce  •Plasma uses 90% of index memory quota  •Need headroom between quota and system memory Memory Management  Plasma Memory Quota  Confidential and Proprietary. Do not distribute without Couchbase consent. © Couchbase 2021. All rights reserved. 6Indexer Runtime  Memory Resident  Index Keys  Internal Data  Structure  Metadata  Buffers  Memory  Fragmentation Index  Memory  Quota Plasma  Quota Confidential and Proprietary. Do not distribute without Couchbase consent. © Couchbase 2021. All rights reserved. 7 Confidential and Proprietary. Do not distribute without Couchbase consent. © Couchbase 2021. All rights reserved. 7•Dynamically adjust plasma quota to create more headroom  •Counteract memory use by internal memory fragmentation and  other processes external to indexer to avoid out-of-memory  scenario  •Tuning algorithm looks at system metrics and is based on  Feedback Control Systems  •System free memory  •Indexer process resident memory  •Decrement if less free memory or resident memory is too high  •Create headroom by evicting memory resident data  •Purge freed memory to OS  •Increment if enough headroom Memory Management  Memory Quota Tuner  Confidential and Proprietary. Do not distribute without Couchbase consent. © Couchbase 2021. All rights reserved. 7Free  Memory Resident  Memory  Desired  Range?  Adjust Quota Confidential and Proprietary. Do not distribute without Couchbase consent. © Couchbase 2021. All rights reserved. 8 Confidential and Proprietary. Do not distribute without Couchbase consent. © Couchbase 2021. All rights reserved. 8 Confidential and Proprietary. Do not distribute without Couchbase consent. © Couchbase 2021. All rights reserved. 8●Bullet level 1, Arial, 16 pt  ○Bullet level 2, Arial, 15 pt  ■Bullet level 3, Arial, 14 pt  Memory Management  Eviction  •Eviction will free memory resident index keys  •During eviction, any dirty index keys will be flushed to disk  •Evict index keys that have not been accessed recently  •Triggers for eviction  •Swapper - background threads that evict pages  •Burst Eviction - runs only when memory usage is greater than  quota and evicts to alleviate memory pressure  •Periodic Eviction - runs continuously in the background, sweeping  index keys to determine and maintain working set in memory  •Eviction during scans and mutations if they throttle under  memory pressure Confidential and Proprietary. Do not distribute without Couchbase consent. © Couchbase 2021. All rights reserved. 9 Confidential and Proprietary. Do not distribute without Couchbase consent. © Couchbase 2021. All rights reserved. 9 Confidential and Proprietary. Do not distribute without Couchbase consent. © Couchbase 2021. All rights reserved. 9●Bullet level 1, Arial, 16 pt  ○Bullet level 2, Arial, 15 pt  ■Bullet level 3, Arial, 14 pt  Memory Management  Swapper - Periodic Eviction  •Determine working set of data to keep in memory  •Treat data accessed in the last 10 minutes as the working  set - called sweep interval  •If determined working set does not fit in quota, proportionally  shorten the sweep interval  •Retain most recently accessed data to exploit temporal locality  •If determined working set fits in quota, preemptively evict to  create headroom for sudden burst of traffic  •Evict till memory usage is 50% of quota  •Evict only if cannot maintain 100% resident ratio Confidential and Proprietary. Do not distribute without Couchbase consent. © Couchbase 2021. All rights reserved. 103Log File Management Confidential and Proprietary. Do not distribute without Couchbase consent. © Couchbase 2021. All rights reserved. 11 Confidential and Proprietary. Do not distribute without Couchbase consent. © Couchbase 2021. All rights reserved. 11 Confidential and Proprietary. Do not distribute without Couchbase consent. © Couchbase 2021. All rights reserved. 1114GB File 0 •A contiguous logical 64-bit log space  •Log Head - offset into first byte of data in LSS  •Log Tail - offset into last byte of data in LSS  •Broken into a list of 4GB physical files  •Each file maps to an offset range in logical log space  •As new data is written (append only), log space grows (Log  Tail advances)  •New physical files added when needed Log File Management  Plasma LSS - Log Structured Store - Logical Log  File 1 File 2 File 3  0GB 4GB 8GB 12GB 16GB File 4  18GB  HEAD TAILFile 3  TAIL TAILConfidential and Proprietary. Do not distribute without Couchbase consent. © Couchbase 2021. All rights reserved. 12 Confidential and Proprietary. Do not distribute without Couchbase consent. © Couchbase 2021. All rights reserved. 12 Confidential and Proprietary. Do not distribute without Couchbase consent. © Couchbase 2021. All rights reserved. 12•Plasma manages index keys in ranges called index page  •As new data is written, an index page may get rewritten  •A new index page is appended to LSS  •Existing index page on file can become stale  •An index page can be rewritten to speed up index scan  •Page Compaction  •Full page persistence  •Fragmentation ratio is the fraction of stale data on disk  •Fragmentation depends on mutation rate  •More new data (higher mutation rate), more data can  become stale Log File Management  Disk Fragmentation Confidential and Proprietary. Do not distribute without Couchbase consent. © Couchbase 2021. All rights reserved. 13 Confidential and Proprietary. Do not distribute without Couchbase consent. © Couchbase 2021. All rights reserved. 13•Log Cleaner - read index pages in LSS from Log Head to Log Tail  •If index page is stale, remove from LSS, advance Log Head  •If index page is still valid, append to LSS, advance Log Head  •As LSS is compacted  •If physical file still has some valid data - shrink file (hole punch)  •If physical file no longer has any valid data - delete file  •Disk Compaction is incremental  •Maintain 30% fragmentation  •Cost is proportional to mutation rate  •If fragmentation is over 80%, throttle write  throughput to avoid running out of disk space HEAD 0GB 2GB HEAD Log File Management  Log Cleaner - Incremental Log Compaction  Confidential and Proprietary. Do not distribute without Couchbase consent. © Couchbase 2021. All rights reserved. 13File 0 File 1 File 2 File 3  4GB 8GB 12GB 16GB File 4  18GB  TAILFile 0  HEAD Confidential and Proprietary. Do not distribute without Couchbase consent. © Couchbase 2021. All rights reserved. 14 Confidential and Proprietary. Do not distribute without Couchbase consent. © Couchbase 2021. All rights reserved. 14 Confidential and Proprietary. Do not distribute without Couchbase consent. © Couchbase 2021. All rights reserved. 14•Hole Punch - reduce file size without deleting the whole file  •Only if physical file has some valid data after disk compaction  •At least 64MB of contiguous stale data  •Only on supported file systems - check at startup  •May not be supported on network and encrypted file systems Log File Management  Plasma Log Cleaner - Hole Punching Confidential and Proprietary. Do not distribute without Couchbase consent. © Couchbase 2021. All rights reserved. 154Scaling Confidential and Proprietary. Do not distribute without Couchbase consent. © Couchbase 2021. All rights reserved. 16 Confidential and Proprietary. Do not distribute without Couchbase consent. © Couchbase 2021. All rights reserved. 16 Confidential and Proprietary. Do not distribute without Couchbase consent. © Couchbase 2021. All rights reserved. 16•Each index runs its own eviction algorithm  •A “hot” index cannot evict an unused page from “cold” index  •Each index allocates its own buffers  •High memory overhead with large number of indexes  •Each index has a dedicated set of “threads” (goroutines)  •High CPU overhead with large number of indexes  •Can hit thread limit per process  •Each index has its own log structured storage (LSS)  •Large number of files with large number of indexes  •High I/O overhead for log maintenance  •Degrade I/O performance with high concurrent traffic Scaling to 10K indexes  Challenges Confidential and Proprietary. Do not distribute without Couchbase consent. © Couchbase 2021. All rights reserved. 17 Confidential and Proprietary. Do not distribute without Couchbase consent. © Couchbase 2021. All rights reserved. 17 Confidential and Proprietary. Do not distribute without Couchbase consent. © Couchbase 2021. All rights reserved. 17•Scaling requires efficient resource management across indexes  •Share resources across indexes  •Resource can be assigned on-the-fly to where demand is needed  •Resource can be reclaimed on-the-fly from where demand has  subsided  •Resource allocation can be elastic  •Improve resource utilization and efficiency  •Resource overhead cannot be proportional to index size  •Resource overhead cannot be proportional to number of  indexes  •Reduce overall memory and I/O overhead  •GSI can support 10K indexes on collections  in 7.0 Scaling to 10K indexes  Solution Confidential and Proprietary. Do not distribute without Couchbase consent. © Couchbase 2021. All rights reserved. 18 Confidential and Proprietary. Do not distribute without Couchbase consent. © Couchbase 2021. All rights reserved. 18 Confidential and Proprietary. Do not distribute without Couchbase consent. © Couchbase 2021. All rights reserved. 18•Indexes are grouped into shards  •Each shard is a unit of resource management  •Share swapper for eviction  •Share background workers (threads/goroutines)  •Share buffer pool  •Share LSS (Collection Index Only) Scaling to 10K indexes  Sharding - Resource Sharing Confidential and Proprietary. Do not distribute without Couchbase consent. © Couchbase 2021. All rights reserved. 19 Confidential and Proprietary. Do not distribute without Couchbase consent. © Couchbase 2021. All rights reserved. 19 Confidential and Proprietary. Do not distribute without Couchbase consent. © Couchbase 2021. All rights reserved. 19•Many enhancements intended for improving efficiency in  resource utilization  •Bloom Filter  •Periodic Eviction and Self-tuning Sweep Interval  •Opportunistic Page Compaction  •Incremental Checkpoint Scaling to 10K indexes  Improve Resource Efficiency Confidential and Proprietary. Do not distribute without Couchbase consent. © Couchbase 2021. All rights reserved. 205I/O Improvements and  Recovery Confidential and Proprietary. Do not distribute without Couchbase consent. © Couchbase 2021. All rights reserved. 21 Confidential and Proprietary. Do not distribute without Couchbase consent. © Couchbase 2021. All rights reserved. 21I/O Improvements  Bloom Filter - Insert Heavy Workloads  Confidential and Proprietary. Do not distribute without Couchbase consent. © Couchbase 2021. All rights reserved. 21Lookup  Old Key  Value  Absent Bloom  Filter •Processing mutation requires two steps  •Lookup and Delete old index key value  •Insert new index key value  •Insert new index key - no need to lookup for old index key value  •Use Bloom Filter to track key value on disk  •Bloom Filter is a space-efficient probabilistic Set data structure  •If old index key value not in Bloom filter, skip lookup old key value from disk  •Reduce random I/O for lookup of old key for insert heavy workload  •Memory usage depends on number of items, resident ratio and configured  accuracy of bloom filter  •Disabled by default and can be turned on and tuned using indexer setting  •For every item on disk, bloom filters consume ~5 bits Confidential and Proprietary. Do not distribute without Couchbase consent. © Couchbase 2021. All rights reserved. 22 Confidential and Proprietary. Do not distribute without Couchbase consent. © Couchbase 2021. All rights reserved. 22I/O Improvements  Opportunistic Page Compaction  Confidential and Proprietary. Do not distribute without Couchbase consent. © Couchbase 2021. All rights reserved. 22•Plasma is MVCC (multi-version concurrency control) for supporting index snapshot  •An index can contain old values, new values and deleted index keys  •Old/deleted key values need to be compacted away in index page when no longer in use  •Preserve memory and disk space  •Reduce I/O needed to read and write page to disk  •Before 7.0, page compaction done when page accumulates N mutations  •Can incur additional I/O for fetching page into memory from disk  •In 7.0.2, page compaction is opportunistic without incurring additional I/O  •When a page is split into two because the page has become full  •When a frequently modified page is written to disk  •When a page is rewritten during incremental disk compaction by log cleaner  •When a page with a lot of old or deleted values is being scanned on (7.1) Confidential and Proprietary. Do not distribute without Couchbase consent. © Couchbase 2021. All rights reserved. 23 Confidential and Proprietary. Do not distribute without Couchbase consent. © Couchbase 2021. All rights reserved. 23I/O Improvements  Incremental Checkpoint  Confidential and Proprietary. Do not distribute without Couchbase consent. © Couchbase 2021. All rights reserved. 23•Indexer creates persistent checkpoint every 10 minutes for crash recovery  •Checkpoint saves index metadata for fast recovery  •Full checkpoint (pre-7.0) persists  •New mutations since previous checkpoint (cost proportional to mutation rate)  •Metadata for all index pages (cost proportional to index size)  •Incremental checkpoint (7.0) improves by persisting only metadata of dirty index pages  since previous checkpoint (cost proportional to mutation rate)  •Reduce CPU and I/O usage with large number indexes (especially “cold” indexes)  •All incremental checkpoints are saved to a log structured storage (append-only)  •Recovery involves reading index metadata from all incremental checkpoints and restoring  the most recent metadata Confidential and Proprietary. Do not distribute without Couchbase consent. © Couchbase 2021. All rights reserved. 24 Confidential and Proprietary. Do not distribute without Couchbase consent. © Couchbase 2021. All rights reserved. 24 Confidential and Proprietary. Do not distribute without Couchbase consent. © Couchbase 2021. All rights reserved. 24●Bullet level 1, Arial, 16 pt  ○Bullet level 2, Arial, 15 pt  ■Bullet level 3, Arial, 14 pt  I/O Improvements  Results - Disk I/O during incremental load  •Disk Utilization improvement  •Lower is better  •Red is 6.6.3  •Blue is 7.0.2 Confidential and Proprietary. Do not distribute without Couchbase consent. © Couchbase 2021. All rights reserved. 256Stats Confidential and Proprietary. Do not distribute without Couchbase consent. © Couchbase 2021. All rights reserved. 26 Confidential and Proprietary. Do not distribute without Couchbase consent. © Couchbase 2021. All rights reserved. 26 Confidential and Proprietary. Do not distribute without Couchbase consent. © Couchbase 2021. All rights reserved. 26•Resident Ratio  •Fraction of index keys present in memory  •Depends on memory quota and working set  •20% is minimum recommended resident ratio  •If goes too low, mutation and scan processing can slow down due  to increased disk I/O. Must revisit sizing to increase the quota  •Index Cache Miss Ratio  •Fraction of needed index keys that were not in memory  •Measure of effectiveness of plasma caching  •If high, indicates that more disk I/O was needed and that the  working set does not fit in memory  •Must revisit the queries or sizing so that more of the working set can  be cached Stats  Index Stats Confidential and Proprietary. Do not distribute without Couchbase consent. © Couchbase 2021. All rights reserved. 27 Confidential and Proprietary. Do not distribute without Couchbase consent. © Couchbase 2021. All rights reserved. 27Stats  Index Stats  Confidential and Proprietary. Do not distribute without Couchbase consent. © Couchbase 2021. All rights reserved. 27•Index Data Size  •Size of data in the index, uncompressed, not including metadata and overheads  •Index Disk Size  •Size of data on disk, compressed, including metadata and overheads like stale data  •If disk size is large compared to data size, check if hole punching is supported and if  fragmentation is being kept in check  •Index Fragmentation  •Percentage of stale data on disk  •Ideally close to 30%. If close to 80%, then the mutation rate can be affected  •Higher resident ratio leaves more disk bandwidth for disk compaction to keep fragmentation in  check Confidential and Proprietary. Do not distribute without Couchbase consent. © Couchbase 2021. All rights reserved.  THANK YOU Confidential and Proprietary. Do not distribute without Couchbase consent. © Couchbase 2021. All rights reserved. 29 Confidential and Proprietary. Do not distribute without Couchbase consent. © Couchbase 2021. All rights reserved. 29 Confidential and Proprietary. Do not distribute without Couchbase consent. © Couchbase 2021. All rights reserved. 29Confidential and Proprietary. Do not distribute without Couchbase consent. © Couchbase 2021. All rights reserved. 30 Confidential and Proprietary. Do not distribute without Couchbase consent. © Couchbase 2021. All rights reserved. 30 Confidential and Proprietary. Do not distribute without Couchbase consent. © Couchbase 2021. All rights reserved. 30Confidential and Proprietary. Do not distribute without Couchbase consent. © Couchbase 2021. All rights reserved. 31 Confidential and Proprietary. Do not distribute without Couchbase consent. © Couchbase 2021. All rights reserved. 31 Confidential and Proprietary. Do not distribute without Couchbase consent. © Couchbase 2021. All rights reserved. 31Confidential and Proprietary. Do not distribute without Couchbase consent. © Couchbase 2021. All rights reserved. 32 Confidential and Proprietary. Do not distribute without Couchbase consent. © Couchbase 2021. All rights reserved. 32 Confidential and Proprietary. Do not distribute without Couchbase consent. © Couchbase 2021. All rights reserved. 32Confidential and Proprietary. Do not distribute without Couchbase consent. © Couchbase 2021. All rights reserved. 33 Confidential and Proprietary. Do not distribute without Couchbase consent. © Couchbase 2021. All rights reserved. 33 Confidential and Proprietary. Do not distribute without Couchbase consent. © Couchbase 2021. All rights reserved. 33Agenda  Fill in agenda items and numbers  Plasma Design  Memory Management  Log File Management  I/O Improvements and Recovery 01/ 01/ 02/ 03/ 04/ 05/Scaling  